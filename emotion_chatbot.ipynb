{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nwsugz/Differentially-Private-Deep-Learning/blob/main/emotion_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9c7112-db7d-4374-84e4-86f5c5e0a9f7",
      "metadata": {
        "id": "af9c7112-db7d-4374-84e4-86f5c5e0a9f7",
        "outputId": "f149b308-2e0e-404a-db60-75158ac140e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\fud10\\anaconda3\\envs\\khb\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\fud10\\anaconda3\\envs\\khb\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter your chat:  나 오늘 뭐할까?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "오늘 슬픔을 느낀다면 다음과 같은 활동을 통해 기분을 나아질 수 있습니다.\n",
            "\n",
            "* **사랑하는 사람에게 연락하세요.** 친구, 가족, 파트너와 이야기하면 기분이 나아지고 연결감을 느낄 수 있습니다.\n",
            "\n",
            "* **자연 속에서 시간 보내기.** 산책이나 하이킹을 나가거나 공원이나 해변을 방문하세요. 자연의 아름다움은 마음을 진정시키고 명확하게 생각하는 데 도움이 됩니다.\n",
            "\n",
            "* **취미 즐기기.** 취미는 우리에게 기쁨과 성취감을 줄 수 있습니다. 그림, 글쓰기, 음악 연주 등 오늘 좋아하는 것을 하세요.\n",
            "\n",
            "* **감사일기 쓰기.** 감사일기에 감사하는 것 세 가지를 매일 쓰면 긍정성과 행복감이 증가하는 것으로 나타났습니다.\n",
            "\n",
            "* **좋아하는 음악 듣기.** 음악은 감정을 표현하고 기분을 바꾸는 강력한 도구입니다. 좋아하는 음악을 듣고 편안해지세요.\n",
            "\n",
            "* **운동하기.** 운동은 엔도르핀을 방출하여 기분을 좋게 합니다. 하루 중 시간을 할애하여 운동을 해 보세요.\n",
            "\n",
            "* **의미 있는 일하기.** 자원봉사를 하거나 가족이나 친구를 도와주는 등 의미 있는 일을 하면 목적 의식과 성취감이 생깁니다.\n",
            "\n",
            "* **전문가의 도움 구하기.** 슬픔이 지속되거나 압도적이라면 상담사나 치료사에게 도움을 구하세요. 전문가는 슬픔을 처리하고 대처 메커니즘을 개발하는 데 도움을 줄 수 있습니다.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from vit_pytorch import ViT\n",
        "from torchvision.models import densenet121\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "import threading\n",
        "\n",
        "# Google API key 설정\n",
        "GOOGLE_API_KEY = \"your_google_api_key\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model_api = genai.GenerativeModel('gemini-pro')\n",
        "chat = model_api.start_chat(history=[])\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "num_classes = 4  # 감정 클래스 수\n",
        "\n",
        "# 데이터 전처리 설정\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 모델 정의: DenseNet-121 + ViT\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.densenet = densenet121(pretrained=False)\n",
        "        num_ftrs = self.densenet.classifier.in_features\n",
        "        self.densenet.classifier = nn.Identity()  # Fully connected layer 제거\n",
        "\n",
        "        self.vit = ViT(\n",
        "            image_size=224,\n",
        "            patch_size=32,\n",
        "            num_classes=1024,  # 중간 차원으로 1024 사용\n",
        "            dim=1024,\n",
        "            depth=6,\n",
        "            heads=16,\n",
        "            mlp_dim=2048,\n",
        "            dropout=0.1,\n",
        "            emb_dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(num_ftrs + 1024, num_classes)  # 결합된 특성에 맞게 조정\n",
        "\n",
        "    def forward(self, x):\n",
        "        densenet_features = self.densenet(x)  # Shape: (batch_size, num_ftrs)\n",
        "        vit_features = self.vit(x)  # Shape: (batch_size, 1024)\n",
        "        combined_features = torch.cat((densenet_features, vit_features), dim=1)  # Shape: (batch_size, num_ftrs + 1024)\n",
        "        out = self.fc(combined_features)\n",
        "        return out\n",
        "\n",
        "# 모델 초기화 및 가중치 로드\n",
        "device = torch.device(\"cpu\")\n",
        "model = HybridModel(num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load('best_hybrid_model_densenet.pth', map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# OpenCV 웹캠 초기화\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# 감정 레이블\n",
        "emotion_labels = ['anger', 'happy', 'panic', 'sadness']\n",
        "\n",
        "# 감정 및 프레임 감지 함수\n",
        "def detect_emotion(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    emotions = []\n",
        "    for (x, y, w, h) in faces:\n",
        "        face_img = frame[y:y+h, x:x+w]\n",
        "        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
        "        face_img = transform(face_img).unsqueeze(0).to(device)\n",
        "        outputs = model(face_img)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        emotion = emotion_labels[predicted.item()]\n",
        "        emotions.append(emotion)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, emotion, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    return emotions, frame\n",
        "\n",
        "# 얼굴 감지기 로드\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# 채팅 입력을 처리하는 스레드 함수\n",
        "def handle_chat_input():\n",
        "    while True:\n",
        "        chat_input = input(\"Enter your chat: \")\n",
        "        if emotions:\n",
        "            emotion_message = f\"My emotion today is {', '.join(emotions)}. {chat_input}\"\n",
        "            response = chat.send_message(emotion_message)\n",
        "            print(response.text)\n",
        "\n",
        "# 채팅 입력 스레드 시작\n",
        "chat_thread = threading.Thread(target=handle_chat_input, daemon=True)\n",
        "chat_thread.start()\n",
        "\n",
        "# 웹캠에서 프레임 읽기\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    emotions, frame = detect_emotion(frame)\n",
        "\n",
        "    # 프레임 표시\n",
        "    cv2.imshow('Emotion Detection', frame)\n",
        "\n",
        "    # 'q' 키를 누르면 종료\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# 종료\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}